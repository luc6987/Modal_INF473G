{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from ipysigma import Sigma\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\i'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\i'\n",
      "/var/folders/0x/wfkkw0fs251d8m1m3p0ks01r0000gn/T/ipykernel_46715/1285495553.py:3: SyntaxWarning: invalid escape sequence '\\i'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWe will do the following steps:\\n    1. extract all the authors as nodes, and the collaboration as edges with the weight of the number of collaborations.\\n    2. extract the title as the name of the paper, the number of arxiv and attribute them to the nodes.\\n    3. find communities in the graph.\\n    5. use citation data to find the most important communities.\\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution for exercise 11\n",
    "\n",
    "\"\"\"\n",
    "explanation of meta data:\n",
    "------------------------------------------------------------------------------\n",
    "\\\\\n",
    "Paper: hep-th/9201018\n",
    "From: OGURAWA@VTCC1.CC.VT.EDU\n",
    "Date: Thu, 9 Jan 1992 18:18:54 -0500 (EST)   (20kb)\n",
    "Date (revised): Mon, 13 Jan 1992 15:35:36 -0500 (EST)\n",
    "\n",
    "Title: Discrete and Continuum Virasoro Constraints in Two-Cut Hermitian Matrix\n",
    "  Models\n",
    "Authors: Waichi Ogura\n",
    "Comments: 25 pages\n",
    "Journal-ref: Prog.Theor.Phys. 89 (1993) 1311-1330\n",
    "\\\\\n",
    "  Continuum Virasoro constraints in the two-cut hermitian matrix models are\n",
    "derived from the discrete Ward identities by means of the mapping from the\n",
    "$GL(\\infty )$ Toda hierarchy to the nonlinear Schr\\\"odinger (NLS) hierarchy.\n",
    "The invariance of the string equation under the NLS flows is worked out. Also\n",
    "the quantization of the integration constant $\\alpha$ reported by Hollowood et\n",
    "al. is explained by the analyticity of the continuum limit.\n",
    "\\\\\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "We will do the following steps:\n",
    "    1. extract all the authors as nodes, and the collaboration as edges with the weight of the number of collaborations.\n",
    "    2. extract the title as the name of the paper, the number of arxiv and attribute them to the nodes.\n",
    "    3. find communities in the graph.\n",
    "    5. use citation data to find the most important communities.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed. Data saved to arxiv_data.json and authors_list.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_author(text):\n",
    "    match = re.search(r'Author:(.*?)(?:\\\\n\\\\n|Report-no:|Comments:|Title:)', text, re.DOTALL)\n",
    "    if match:\n",
    "        authors = match.group(1).replace(\"\\n\", \" \")  # 处理换行符\n",
    "        authors = re.sub(r'\\(.*?\\)', '', authors)  # 去除括号及其内容\n",
    "        authors = [a.strip() for a in re.split(r',| and ', authors)]  # 拆分多个作者\n",
    "        authors = [re.sub(r'[^a-zA-Z .]', '', a) for a in authors]  # 只保留拉丁字母、空格和点\n",
    "        authors = [re.sub(r'\\s+', ' ', a) for a in authors]  # 合并多个空格\n",
    "        #将所有词的首字母大写, 其他字母小写\n",
    "        authors = [a.title() for a in authors]\n",
    "        authors = [a.replace(\"Nieegawa\", \"Niegawa\") for a in authors]  # 统一名字格式\n",
    "        authors = merge_name_variants(authors)  # 统一名字变体\n",
    "        authors = resolve_abbreviations(authors)  # 处理缩写和全称重复\n",
    "        authors = list(filter(None, authors))  # 去除空项\n",
    "        return sorted(authors)\n",
    "    return []\n",
    "\n",
    "def merge_name_variants(authors):\n",
    "    \"\"\"合并同一作者的不同写法，如 Juan Maldacena 和 Juan M. Maldacena\"\"\"\n",
    "    name_variants = {}\n",
    "    \n",
    "    for author in authors:\n",
    "        base_name = re.sub(r'\\b[A-Z]\\.', '', author).strip()  # 移除首字母缩写\n",
    "        if base_name in name_variants:\n",
    "            name_variants[base_name].add(author)\n",
    "        else:\n",
    "            name_variants[base_name] = {author}\n",
    "    \n",
    "    return [max(variants, key=len) for variants in name_variants.values()]\n",
    "\n",
    "def resolve_abbreviations(authors):\n",
    "    \"\"\"检查是否有缩写和全称重复的情况，若有，则保留全称\"\"\"\n",
    "    full_names = set()\n",
    "    abbreviations = set()\n",
    "    \n",
    "    for author in authors:\n",
    "        if '.' in author:\n",
    "            abbreviations.add(author)\n",
    "        else:\n",
    "            full_names.add(author)\n",
    "    \n",
    "    # 只保留非缩写形式的作者名\n",
    "    final_authors = full_names.union(a for a in abbreviations if a not in full_names)\n",
    "    return list(final_authors)\n",
    "\n",
    "def extract_arxiv_info(abs_file):\n",
    "    with open(abs_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    paper_id_match = re.search(r'Paper:\\s*hep-th/(\\d+)', content)\n",
    "    title_match = re.search(r'Title:\\s*(.+)', content)\n",
    "    authors = extract_author(content)\n",
    "    \n",
    "    if paper_id_match and title_match and authors:\n",
    "        return paper_id_match.group(1), {\n",
    "            \"title\": title_match.group(1),\n",
    "            \"authors\": authors\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def process_abstracts(directory):\n",
    "    arxiv_data = {}\n",
    "    authors_set = set()\n",
    "    \n",
    "    for year in os.listdir(directory):\n",
    "        year_path = os.path.join(directory, year)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue\n",
    "        \n",
    "        for file in os.listdir(year_path):\n",
    "            if file.endswith(\".abs\"):\n",
    "                abs_path = os.path.join(year_path, file)\n",
    "                result = extract_arxiv_info(abs_path)\n",
    "                if result:\n",
    "                    arxiv_id, data = result\n",
    "                    arxiv_data[arxiv_id] = data\n",
    "                    authors_set.update(data[\"authors\"])\n",
    "    \n",
    "    with open(\"arxiv_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(arxiv_data, json_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    sorted_authors = sorted(authors_set)\n",
    "    with open(\"authors_list.csv\", \"w\", encoding=\"utf-8\", newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"Author\"])\n",
    "        for author in sorted_authors:\n",
    "            writer.writerow([author])\n",
    "    \n",
    "    print(\"Extraction completed. Data saved to arxiv_data.json and authors_list.csv\")\n",
    "\n",
    "# 示例调用\n",
    "process_abstracts(\"./cit-HepTh-abstracts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Name 1                    Name 2  Similarity\n",
      "0               A. Strominger           Andy Strominger        85.7\n",
      "1           Andrew Strominger           Andy Strominger        87.5\n",
      "2                  B. Schroer             B.J. Schroers        87.0\n",
      "3                H. Samtleben          J.A.H. Samtleben        85.7\n",
      "4                   H. Suzuki                 S. Suzuki        88.9\n",
      "5             John H. Schwarz              John Schwarz        88.9\n",
      "6                Myck Schwetz           Myckola Schwetz        88.9\n",
      "7              N. I. Stoilova               N. Stoilova        88.0\n",
      "8               R. Schimmrigk           Rolf Schimmrigk        85.7\n",
      "9              Igor A. Bandos               Igor Bandos        88.0\n",
      "10             J. Bockenhauer          Jens Bockenhauer        86.7\n",
      "11               Thomas Banks                 Tom Banks        85.7\n",
      "12               Andrey Dubin          Andrey Yu. Dubin        85.7\n",
      "13                Brian Dolan            Brian P. Dolan        88.0\n",
      "14            I. G. Korepanov         Igor G. Korepanov        87.5\n",
      "15              A. V. Antonov              D.V. Antonov        88.0\n",
      "16            A. Yu. Alekseev             A.I. Alekseev        85.7\n",
      "17            D. V. Ahluwalia           K. S. Ahluwalia        86.7\n",
      "18             I. G. Avramidi          Ivan G. Avramidi        86.7\n",
      "19                 J. Ambjorn               Jan Ambjorn        85.7\n",
      "20              Jeeva Anandan          Jeeva S. Anandan        89.7\n",
      "21          H.R. Christiansen      Hugo R. Christiansen        86.5\n",
      "22           David B. Fairlie             David Fairlie        89.7\n",
      "23             Franco Ferrari             Frank Ferrari        88.9\n",
      "24           A. G. Ushveridze             A. Ushveridze        89.7\n",
      "25                  A. Gorsky              A. S. Gorsky        85.7\n",
      "26                David Gross            David J. Gross        88.0\n",
      "27              G. W. Gibbons              G.W. Gibbons        96.0\n",
      "28           H. Garciacompean        Hugo Garciacompean        88.2\n",
      "29           Michael B. Green             Michael Green        89.7\n",
      "30                   Boyu Hou                Boyuan Hou        88.9\n",
      "31                  D. Ivanov                 S. Ivanov        88.9\n",
      "32  Aleksey Yu. Nurmagambetov  Alexei Yu. Nurmagambetov        89.8\n",
      "33               Denis Juriev           Denis V. Juriev        88.9\n",
      "34               C. P. Martin                 P. Martin        85.7\n",
      "35                J. Mccarthy              Jim Mccarthy        87.0\n",
      "36           N. E. Mavromatos        Nick E. Mavromatos        88.2\n",
      "37               Yu. Makeenko             Yuri Makeenko        88.0\n",
      "38               M. V. Zyskin                 M. Zyskin        85.7\n",
      "39                  M. Tanaka                 T. Tanaka        88.9\n",
      "40                 M. Welling               Max Welling        85.7\n",
      "41           Robert A. Weston             Robert Weston        89.7\n"
     ]
    }
   ],
   "source": [
    "# to clean the authors list \n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "import itertools\n",
    "\n",
    "# ========== 读取数据 ==========\n",
    "file_path = \"/Users/yuguang/Projet/modal/Modal_INF473G/authors_list.csv\"\n",
    "similarity_threshold = 0.85\n",
    "\n",
    "# ========== 读取数据 ==========\n",
    "df = pd.read_csv(file_path)\n",
    "authors = df['Author'].dropna().unique()\n",
    "authors_clean = [a.strip() for a in authors]\n",
    "\n",
    "# ========== 相似度函数 ==========\n",
    "def similarity_ratio(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "# ========== 找出相似名字并保留最长 ==========\n",
    "# 先做完全相同/非常相似的名字分组\n",
    "groups_similar = []\n",
    "\n",
    "used = set()\n",
    "for i, name1 in enumerate(authors_clean):\n",
    "    if name1 in used:\n",
    "        continue\n",
    "    group = [name1]\n",
    "    used.add(name1)\n",
    "    for name2 in authors_clean[i+1:]:\n",
    "        if name2 not in used and similarity_ratio(name1, name2) >= 0.90:\n",
    "            group.append(name2)\n",
    "            used.add(name2)\n",
    "    groups_similar.append(group)\n",
    "\n",
    "# 每组保留最长的名字\n",
    "cleaned_names = []\n",
    "for group in groups_similar:\n",
    "    longest = max(group, key=len)\n",
    "    cleaned_names.append(longest)\n",
    "\n",
    "# ========== 按姓氏首字母分组 ==========\n",
    "groups = defaultdict(list)\n",
    "for name in cleaned_names:\n",
    "    last_name = name.split()[-1] if name.split() else ''\n",
    "    key = last_name[0].lower() if last_name else 'other'\n",
    "    groups[key].append(name)\n",
    "\n",
    "# ========== 分组内模糊匹配 ==========\n",
    "grouped_possible_duplicates = []\n",
    "for key, names in groups.items():\n",
    "    for a1, a2 in itertools.combinations(names, 2):\n",
    "        similarity = similarity_ratio(a1, a2)\n",
    "        if similarity >= similarity_threshold and a1 != a2:\n",
    "            grouped_possible_duplicates.append((a1, a2, round(similarity * 100, 1)))\n",
    "\n",
    "# ========== 结果整理 ==========\n",
    "grouped_fuzzy_matches = pd.DataFrame(\n",
    "    grouped_possible_duplicates,\n",
    "    columns=['Name 1', 'Name 2', 'Similarity']\n",
    ")\n",
    "\n",
    "# ========== 输出 ==========\n",
    "print(grouped_fuzzy_matches)\n",
    "\n",
    "pd.DataFrame({'Cleaned Names': cleaned_names}).to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "example of Cit-HepTh.txt:\n",
    "# Directed graph (each unordered pair of nodes is saved once): Cit-HepTh.txt \n",
    "# Paper citation network of Arxiv High Energy Physics Theory category\n",
    "# Nodes: 27770 Edges: 352807\n",
    "# FromNodeId\tToNodeId\n",
    "0001001\t9304045\n",
    "0001001\t9308122\n",
    "0001001\t9309097\n",
    "0001001\t9311042\n",
    "0001001\t9401139\n",
    "\n",
    "for each paper, we will add a new attribute of list that shows all the papers that this paper cite, with its arxiv number.\n",
    "We will add it iff both papers of a citation are in the paper. And save it to arxiv_papers.json.\n",
    "\n",
    "example of papers_standardized.json\n",
    "\n",
    "  {\n",
    "    \"paper_id\": \"hep-th/9211063\",\n",
    "    \"from\": \"Malcolm Perry <M.J.Perry@damtp.cambridge.ac.uk>\",\n",
    "    \"submitted\": \"Sat, 14 Nov 92 17:58:35 GMT (27kb)\",\n",
    "    \"title\": \"Topological Conformal Gravity in Four Dimensions\",\n",
    "    \"authors\": [\n",
    "      \"Malcolm J. Perry\",\n",
    "      \"Edward Teo\"\n",
    "    ],\n",
    "    \"comments\": \"35 pages, harvmac, DAMTP R92/42\",\n",
    "    \"report_no\": null,\n",
    "    \"journal_ref\": \"Nucl.Phys. B401 (1993) 206-238\",\n",
    "    \"subject_class\": null,\n",
    "    \"proxy\": null,\n",
    "    \"abstract\": \"In this paper, we present a new formulation of topological conformal gravity in four dimensions. Such a theory was first considered by Witten as a possible gravitational counterpart of topological Yang-Mills theory, but several problems left it incomplete. The key in our approach is to realise a theory which describes deformations of conformally self-dual gravitational instantons. We first identify the appropriate elliptic complex which does precisely this. By applying the Atiyah-Singer index theorem, we calculate the number of independent deformations of a given gravitational instanton which preserve its self-duality. We then quantise topological conformal gravity by BRST gauge-fixing, and discover how the quantum theory is naturally described by the above complex. Indeed, it is a process which closely parallels that of the Yang-Mills theory, and we show how the partition function generates an uncanny gravitational analogue of the first Donaldson invariant.\",\n",
    "    \"year\": 1992\n",
    "  },\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#load data from papers_standardized.json\n",
    "with open(\"papers_standardized.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    papers = {paper[\"paper_id\"]: paper for paper in json.load(f)}\n",
    "\n",
    "\n",
    "# Load the citation data\n",
    "citations = {}\n",
    "with open(\"Cit-HepTh.txt\", \"r\") as f:\n",
    "    # Skip the lines with comments\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line.startswith(\"#\"):\n",
    "            break\n",
    "    for line in f:\n",
    "        source, target = map(int, line.strip().split())\n",
    "        source = str(source)\n",
    "        target = str(target)\n",
    "        if source in papers and target in papers:\n",
    "            if source not in citations:\n",
    "                citations[source] = []\n",
    "            citations[source].append(target)\n",
    "\n",
    "# Add the citation of the papers and the paper cited by the papers\n",
    "for paper_id, data in papers.items():\n",
    "    if paper_id in citations:\n",
    "        data[\"citations\"] = citations[paper_id]\n",
    "    else:\n",
    "        data[\"citations\"] = []\n",
    "\n",
    "\n",
    "# Save the updated data\n",
    "with open(\"arxiv_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(papers, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 8187\n",
      "Number of edges: 19205\n"
     ]
    }
   ],
   "source": [
    "# create a graph with authors as nodes and collaborations as edges\n",
    "import networkx as nx\n",
    "Ghep = nx.Graph()\n",
    "for paper in papers.values():\n",
    "    for author in paper[\"authors\"]:\n",
    "        if author not in Ghep:\n",
    "            Ghep.add_node(author, papers=[])\n",
    "            # Add the paper with its arxiv numbres to the author's list \n",
    "        Ghep.nodes[author][\"papers\"].append(paper[\"title\"])\n",
    "\n",
    "    for author1 in paper[\"authors\"]:\n",
    "        for author2 in paper[\"authors\"]:\n",
    "            if author1 != author2:\n",
    "                if not Ghep.has_edge(author1, author2):\n",
    "                    Ghep.add_edge(author1, author2, weight=0)\n",
    "                Ghep[author1][author2][\"weight\"] += 1\n",
    "\n",
    "print(f\"Number of nodes: {Ghep.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {Ghep.number_of_edges()}\")\n",
    "\n",
    "#print all the authors with the number of collaborations in authors.csv in alphabetical order\n",
    "with open(\"authors_list.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Author,Number of collaborations\\n\")\n",
    "    for author in sorted(Ghep.nodes):\n",
    "        f.write(f\"{author},{Ghep.degree(author)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1542\n",
      "1161\n",
      "1994\n"
     ]
    }
   ],
   "source": [
    "# find communities in the graph\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "\n",
    "# Louvain community detection\n",
    "lc_Ghep = louvain_communities(Ghep,resolution=40.0, seed=123)\n",
    "\n",
    "#print the community number\n",
    "print(len(lc_Ghep))\n",
    "# Clique percolation method\n",
    "from networkx.algorithms.community import k_clique_communities\n",
    "cp_Ghep= k_clique_communities(Ghep,3)\n",
    "#print the community number\n",
    "print(len(list(cp_Ghep)))\n",
    "\n",
    "\n",
    "\n",
    "# label propagation algorithm\n",
    "from networkx.algorithms.community.label_propagation import label_propagation_communities\n",
    "lp_Ghep = list(label_propagation_communities(Ghep))\n",
    "#print the community number\n",
    "print(len(lp_Ghep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important communities:\n",
      "Community 1: 23 authors, 3146.0 citations\n",
      "  Ofer Aharony\n",
      "  Y. Kinar\n",
      "  Morten Krogh\n",
      "  S. Yankielowizc\n",
      "  A. Loewy\n",
      "  Ori J. Ganor\n",
      "  Y. Artstein\n",
      "  Mordechai Spiegelglas\n",
      "  Joanna L. Karczmarek\n",
      "  Aaron Bergman\n",
      "  Andreas Brandhuber\n",
      "  Adi Armoni\n",
      "  C. Sonnenschein\n",
      "  Yitzhak Frishman\n",
      "  N. Sochen J. Sonnenschein\n",
      "  Yoav Lavi\n",
      "  Michael E. Peskin\n",
      "  Eugene A. Mirabelli\n",
      "  Shimon Yankielowicz\n",
      "  Vadim S. Kaplunovsky\n",
      "  Nissan Itzhaki\n",
      "  Shlomo S. Razamat\n",
      "  Ehud Schreiber\n",
      "Community 2: 14 authors, 2179.0 citations\n",
      "  Mirjam Cvetiv C\n",
      "  Jose R. Espinosa\n",
      "  R. L. Davis\n",
      "  Mirjam Cvetic\n",
      "  Harald H. Soleng\n",
      "  Philip J. Rosenthal\n",
      "  Stephen Griffies\n",
      "  Paul Langacker\n",
      "  Mirjam Cvetivc\n",
      "  Gerald B. Cleaver\n",
      "  David C. Lewellen\n",
      "  Lisa Everett\n",
      "  Donam Youm\n",
      "  Kwanleung Chan\n",
      "Community 3: 15 authors, 2129.0 citations\n",
      "  Shivaji L. Sondhi\n",
      "  Arkadas Ozakin\n",
      "  Akikazu Hashimoto\n",
      "  Curtis G. Callan\n",
      "  Peter Ouyang\n",
      "  Steven S. Gubser\n",
      "  Vyacheslav S. Rychkov\n",
      "  Christof Schmidhuber\n",
      "  A. M. Polyakov\n",
      "  Ali Yegulalp\n",
      "  Krev Simir Demeterfi\n",
      "  G.V. Bhanot\n",
      "  Michael Krasnitz\n",
      "  Igor R. Klebanov\n",
      "  Christopher P. Herzog\n",
      "Community 4: 11 authors, 1927.0 citations\n",
      "  Jeremy Michelson\n",
      "  Guowen Peng\n",
      "  Daniele Amati\n",
      "  Arkady A. Tseytlin\n",
      "  Jorge G. Russo\n",
      "  Junseong Heo\n",
      "  Hongya Liu\n",
      "  Iouri Chepelev\n",
      "  Lawrence M. Krauss\n",
      "  Angelos Fotopoulos\n",
      "  R.R. Metsaev\n",
      "Community 5: 16 authors, 1895.0 citations\n",
      "  Sabbir Rahman\n",
      "  Jaydeep Majumder\n",
      "  M. C. Bento\n",
      "  Amer Iqbal\n",
      "  Hidenori Sonoda\n",
      "  Wangchang Su\n",
      "  O. Bertolami\n",
      "  Barton Zwiebach\n",
      "  Anjan Ananda Sen\n",
      "  Oliver Dewolfe\n",
      "  Alexander Belopolsky\n",
      "  K. Ranganathan\n",
      "  Leonardo Rastelli\n",
      "  R. Saroja\n",
      "  Davide Gaiotto\n",
      "  Tamas Hauer\n",
      "Community 6: 13 authors, 1856.0 citations\n",
      "  S. Ferrara Amd M. A. Lledo\n",
      "  Riccardo Dauria\n",
      "  T. Magri\n",
      "  A.C. Cadavid\n",
      "  T. Regge\n",
      "  V.S.V. Varadarajan\n",
      "  M. Villasante\n",
      "  Floriana Gargiulo\n",
      "  Sergio Ferrara\n",
      "  Silvia Vaula\n",
      "  Anna Ceresole\n",
      "  A. Resturccia\n",
      "  Laura Andrianopoli\n",
      "Community 7: 20 authors, 1617.0 citations\n",
      "  Paul F. Mende\n",
      "  Yun Soo Myung\n",
      "  Georgios Metikas\n",
      "  Bugra Borasoy\n",
      "  Tzedan Chung\n",
      "  Yoji Michishita\n",
      "  Gungwon Kang\n",
      "  Kyoungtae Kimm\n",
      "  Seoktae Koh\n",
      "  Jin Hur\n",
      "  Hsienchung Kao\n",
      "  Erick A. Roura\n",
      "  Pablo J. Marrero\n",
      "  Minyoung Choi\n",
      "  Yuval Neeman\n",
      "  Piljin Yi\n",
      "  W.F. Chen. H.C. Lee\n",
      "  Branko Urosevic\n",
      "  W. S. Lyi\n",
      "  Nathan Salwen\n",
      "Community 8: 17 authors, 1487.0 citations\n",
      "  Frederic Leblond\n",
      "  Kenneth J. Lovis\n",
      "  Robert C. Myers\n",
      "  Laur Jarv\n",
      "  G. Michaud\n",
      "  J. C. Breckenridge\n",
      "  J.D. Cohn\n",
      "  David C. Page\n",
      "  Vipul Periwal\n",
      "  David J. Winters\n",
      "  Clifford V. Johnson\n",
      "  Hoseong La\n",
      "  Luc Marleau\n",
      "  O Yvind Tafjord\n",
      "  Ramzi R. Khuri\n",
      "  J.R. Anglin\n",
      "  Adel M. Awad\n",
      "Community 9: 16 authors, 1456.0 citations\n",
      "  Byungkoo Lee\n",
      "  Jiro Hashiba\n",
      "  Kazuhiro Sakai\n",
      "  Yasuhiko Yamada\n",
      "  Katsushi Ito\n",
      "  Takeo Araki\n",
      "  Tohru Eguchi\n",
      "  Kenji Mohri\n",
      "  Seiji Terashima\n",
      "  Koichi Yoshioka\n",
      "  Huanxiong Yang\n",
      "  Yoko Onjo\n",
      "  Ianwoo Kim\n",
      "  Hiroaki Kanno\n",
      "  Kyungseok Cha\n",
      "  Toshiya Kawai\n",
      "Community 10: 12 authors, 1408.0 citations\n",
      "  Giuseppe Dappollonio\n",
      "  Massimo Bianchi\n",
      "  Gianfranco Pradisi\n",
      "  Carlo Angelantonj\n",
      "  Giancarlo Rossi\n",
      "  Fabio Riccioni\n",
      "  Jihad Mourad\n",
      "  Emilian Dudas\n",
      "  Yassen S. Stanev\n",
      "  C. Timirgaziu\n",
      "  Stefano Kovacs\n",
      "  Augusto Sagnotti\n"
     ]
    }
   ],
   "source": [
    "#calculate the net citations numbers between communities directly from the arxiv_papers.json\n",
    "\n",
    "\"\"\"\n",
    "First few lines of the arxiv_papers.json:\n",
    "{\n",
    "    \"9203077\": {\n",
    "        \"title\": \"Finite W-algebras\",\n",
    "        \"authors\": [\n",
    "            \"T.Tjin\"\n",
    "        ],\n",
    "        \"citations\": 13,\n",
    "        \"cited_by\": []\n",
    "    },\n",
    "    \"9203063\": {\n",
    "        \"title\": \"The Spectrum of Sl(2, R)/U(1) Black Hole Conformal Field Theory\",\n",
    "        \"authors\": [\n",
    "            \"Dileep P. Jatkar\"\n",
    "        ],\n",
    "        \"citations\": 0,\n",
    "        \"cited_by\": []\n",
    "    },\n",
    "    \"9212146\": {\n",
    "\"\"\"\n",
    "communities = list(lc_Ghep)  # Use the Louvain communities\n",
    "# Load the papers data\n",
    "with open(\"arxiv_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "# Create a dictionary of authors to communities\n",
    "author_community = {}\n",
    "for i, community in enumerate(communities):\n",
    "    for author in community:\n",
    "        author_community[author] = i\n",
    "\n",
    "# Calculate the net citations between communities\n",
    "net_citations = np.zeros((len(communities), len(communities)))\n",
    "for paper in papers.values():\n",
    "    if \"citations\" not in paper:\n",
    "        continue\n",
    "    # Ensure the source author exists in the author_community dictionary\n",
    "    if paper[\"authors\"]:\n",
    "        source_community = author_community.get(paper[\"authors\"][0], -1)\n",
    "        if source_community != -1:\n",
    "            for target in paper[\"citations\"]:\n",
    "                # Ensure the target paper exists and has authors\n",
    "                if target in papers and papers[target][\"authors\"]:\n",
    "                    target_community = author_community.get(papers[target][\"authors\"][0], -1)\n",
    "                    if target_community != -1:\n",
    "                        net_citations[source_community, target_community] += 1\n",
    "\n",
    "\n",
    "# Find the most important communities by net citations/\n",
    "community_citations = net_citations.sum(axis=1)\n",
    "most_important_communities = np.argsort(community_citations)[::-1]  \n",
    "\n",
    "print(\"Most important communities:\")\n",
    "for i in range(10):\n",
    "    community = communities[most_important_communities[i]]\n",
    "    print(f\"Community {i + 1}: {len(community)} authors, {community_citations[most_important_communities[i]]} citations\")\n",
    "    for author in community:\n",
    "        print(f\"  {author}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1542\n",
      "Number of edges: 13053\n"
     ]
    }
   ],
   "source": [
    "#create a graph of communities as summation of the information of the authors named after the most important author in the community.\n",
    "# we will create a graph of communities as summation of the information of the authors named after the most important author in the community. \n",
    "# The number of citations as the weight of the edges.\n",
    "G_communities = nx.Graph()\n",
    "for i, community in enumerate(communities):\n",
    "    most_important_author = max(community, key=lambda x: len(papers[x][\"papers\"]) if x in papers else 0)\n",
    "    G_communities.add_node(i, name=most_important_author, size=len(community))\n",
    "    #add the number of members in the community as the size of the node\n",
    "    for author in community:\n",
    "        if author in papers:\n",
    "            G_communities.nodes[i][author] = papers[author][\"papers\"]\n",
    "    #add the total citations in the community\n",
    "    G_communities.nodes[i][\"total_citations\"] = community_citations[i]\n",
    "\n",
    "# Create the edges between communities based on the net citations\n",
    "for i in range(len(communities)):\n",
    "    for j in range(i + 1, len(communities)):\n",
    "        # Add the number of citations as the weight of the edge with weight of the number of citations\n",
    "        if net_citations[i][j] > 0:\n",
    "            G_communities.add_edge(i, j, weight=net_citations[i][j])\n",
    "\n",
    "print(f\"Number of nodes: {G_communities.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G_communities.number_of_edges()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0x/wfkkw0fs251d8m1m3p0ks01r0000gn/T/ipykernel_1771/1899590505.py:16: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  colormap = cm.get_cmap('tab10', num_comms)  # 可换 'tab20'、'plasma' 等\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506ba4d4a02b4c99a7e8d4333e180487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sigma(nx.Graph with 1,542 nodes and 13,053 edges)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from ipysigma import Sigma\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------- ① 社区编号：把列表 communities 转换为 {node: community_id} 字典 --------\n",
    "\n",
    "community_dict = {}\n",
    "for comm_id, comm_nodes in enumerate(communities):\n",
    "    for node in comm_nodes:\n",
    "        community_dict[node] = comm_id\n",
    "\n",
    "num_comms = len(set(community_dict.values()))\n",
    "colormap = cm.get_cmap('tab10', num_comms)  # 可换 'tab20'、'plasma' 等\n",
    "\n",
    "# -------- ② 统计每个社区总引用 --------\n",
    "\n",
    "community_citations = defaultdict(int)\n",
    "for node in G_communities.nodes:\n",
    "    comm_id = community_dict.get(node, -1)\n",
    "    community_citations[comm_id] += G_communities.nodes[node][\"total_citations\"]\n",
    "\n",
    "max_cit = max(community_citations.values())\n",
    "\n",
    "# -------- ③ 给节点分配颜色（颜色亮度 = 社区总引用） --------\n",
    "\n",
    "for node in G_communities.nodes:\n",
    "    comm_id = community_dict.get(node, -1)\n",
    "\n",
    "    if comm_id == -1:\n",
    "        G_communities.nodes[node][\"colors\"] = \"#cccccc\"  # 未分组灰色\n",
    "    else:\n",
    "        base_color = colormap(comm_id % num_comms)\n",
    "        brightness = community_citations[comm_id] / max_cit\n",
    "        r, g, b, _ = base_color\n",
    "        # 增加最低亮度，避免太暗\n",
    "        r = min(1.0, r * brightness + 0.3)\n",
    "        g = min(1.0, g * brightness + 0.3)\n",
    "        b = min(1.0, b * brightness + 0.3)\n",
    "        G_communities.nodes[node][\"colors\"] = mcolors.to_hex((r, g, b))\n",
    "\n",
    "# -------- ④ 边颜色：跨社区边浅灰，同社区边深色 --------\n",
    "\n",
    "for i, j in G_communities.edges:\n",
    "    comm_i = community_dict.get(i, -1)\n",
    "    comm_j = community_dict.get(j, -1)\n",
    "\n",
    "    if comm_i != comm_j:\n",
    "        G_communities.edges[i, j][\"color\"] = \"rgba(200,200,200,0.1)\"  # 淡灰\n",
    "    else:\n",
    "        G_communities.edges[i, j][\"color\"] = \"rgba(50,50,50,0.6)\"      # 深色\n",
    "\n",
    "# -------- ⑤ 社区分开布局：圆周排列 --------\n",
    "\n",
    "# 圆周上为每个社区分配中心位置\n",
    "angle = np.linspace(0, 2 * np.pi, num_comms + 1)\n",
    "community_positions = {}\n",
    "for comm_id in range(num_comms):\n",
    "    x = 10 * np.cos(angle[comm_id])\n",
    "    y = 10 * np.sin(angle[comm_id])\n",
    "    community_positions[comm_id] = (x, y)\n",
    "\n",
    "# 对每个社区分别布局并移动\n",
    "for comm_id in range(num_comms):\n",
    "    nodes_in_comm = [n for n in G_communities.nodes if community_dict.get(n, -1) == comm_id]\n",
    "    subgraph = G_communities.subgraph(nodes_in_comm)\n",
    "\n",
    "    # 社区内部布局\n",
    "    pos_sub = nx.spring_layout(subgraph, k=0.5, seed=comm_id)\n",
    "\n",
    "    # 圆周位置\n",
    "    cx, cy = community_positions[comm_id]\n",
    "\n",
    "    for n in subgraph.nodes:\n",
    "        G_communities.nodes[n][\"x\"] = pos_sub[n][0] + cx\n",
    "        G_communities.nodes[n][\"y\"] = pos_sub[n][1] + cy\n",
    "\n",
    "# -------- ⑥ 节点大小：按 total_citations 控制 --------\n",
    "\n",
    "for node in G_communities.nodes:\n",
    "    G_communities.nodes[node][\"size\"] = G_communities.nodes[node][\"total_citations\"] / 100\n",
    "\n",
    "# -------- ⑦ 标签：只显示引用高的节点 --------\n",
    "\n",
    "for node in G_communities.nodes:\n",
    "    if G_communities.nodes[node][\"total_citations\"] > 500:\n",
    "        G_communities.nodes[node][\"label\"] = G_communities.nodes[node][\"name\"]\n",
    "    else:\n",
    "        G_communities.nodes[node][\"label\"] = \"\"\n",
    "\n",
    "# -------- ⑧ 可视化：Sigma --------\n",
    "\n",
    "sigma = Sigma(\n",
    "    G_communities,\n",
    "    node_color=\"colors\",\n",
    "    edge_color=\"color\",\n",
    "    node_label=\"label\",\n",
    "    node_size=\"size\",\n",
    "    label_font=\"cursive\",\n",
    "    default_edge_type=\"curve\"\n",
    ")\n",
    "\n",
    "sigma\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
