{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Read a research article that investigates the potential of using modularity to detect polarization\n",
    "\n",
    "You will find on Moodle a research article published at the conference ICWSM https://www.icwsm.org/2025/index.html\n",
    "Take the time to read it and see how researchers test ideas on real applications and propose new metrics when they consider existing tools are not sufficient. In your project you will also have to investigate your hypothesis on the data of your choice! \n",
    "\n",
    "11. After reading the article, implement the notion of boundary described. Apply it on one of the graphs you have seen so far in this class, or on one graph from here (choose smaller graphs for efficiency): http://snap.stanford.edu/data/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from ipysigma import Sigma\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\i'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\i'\n",
      "/var/folders/0x/wfkkw0fs251d8m1m3p0ks01r0000gn/T/ipykernel_43807/1285495553.py:3: SyntaxWarning: invalid escape sequence '\\i'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWe will do the following steps:\\n    1. extract all the authors as nodes, and the collaboration as edges with the weight of the number of collaborations.\\n    2. extract the title as the name of the paper, the number of arxiv and attribute them to the nodes.\\n    3. find communities in the graph.\\n    5. use citation data to find the most important communities.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution for exercise 11\n",
    "\n",
    "\"\"\"\n",
    "explanation of meta data:\n",
    "------------------------------------------------------------------------------\n",
    "\\\\\n",
    "Paper: hep-th/9201018\n",
    "From: OGURAWA@VTCC1.CC.VT.EDU\n",
    "Date: Thu, 9 Jan 1992 18:18:54 -0500 (EST)   (20kb)\n",
    "Date (revised): Mon, 13 Jan 1992 15:35:36 -0500 (EST)\n",
    "\n",
    "Title: Discrete and Continuum Virasoro Constraints in Two-Cut Hermitian Matrix\n",
    "  Models\n",
    "Authors: Waichi Ogura\n",
    "Comments: 25 pages\n",
    "Journal-ref: Prog.Theor.Phys. 89 (1993) 1311-1330\n",
    "\\\\\n",
    "  Continuum Virasoro constraints in the two-cut hermitian matrix models are\n",
    "derived from the discrete Ward identities by means of the mapping from the\n",
    "$GL(\\infty )$ Toda hierarchy to the nonlinear Schr\\\"odinger (NLS) hierarchy.\n",
    "The invariance of the string equation under the NLS flows is worked out. Also\n",
    "the quantization of the integration constant $\\alpha$ reported by Hollowood et\n",
    "al. is explained by the analyticity of the continuum limit.\n",
    "\\\\\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "We will do the following steps:\n",
    "    1. extract all the authors as nodes, and the collaboration as edges with the weight of the number of collaborations.\n",
    "    2. extract the title as the name of the paper, the number of arxiv and attribute them to the nodes.\n",
    "    3. find communities in the graph.\n",
    "    5. use citation data to find the most important communities.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed. Data saved to arxiv_data.json and authors_list.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def extract_author(text):\n",
    "    match = re.search(r'Author:(.*?)(?:\\\\n\\\\n|Report-no:|Comments:|Title:)', text, re.DOTALL)\n",
    "    if match:\n",
    "        authors = match.group(1).replace(\"\\n\", \" \")  # 处理换行符\n",
    "        authors = re.sub(r'\\(.*?\\)', '', authors)  # 去除括号及其内容\n",
    "        authors = [a.strip() for a in re.split(r',| and ', authors)]  # 拆分多个作者\n",
    "        authors = [re.sub(r'[^a-zA-Z .]', '', a) for a in authors]  # 只保留拉丁字母、空格和点\n",
    "        authors = [re.sub(r'\\s+', ' ', a) for a in authors]  # 合并多个空格\n",
    "        #将所有词的首字母大写, 其他字母小写\n",
    "        authors = [a.title() for a in authors]\n",
    "        authors = [a.replace(\"Nieegawa\", \"Niegawa\") for a in authors]  # 统一名字格式\n",
    "        authors = merge_name_variants(authors)  # 统一名字变体\n",
    "        authors = resolve_abbreviations(authors)  # 处理缩写和全称重复\n",
    "        authors = list(filter(None, authors))  # 去除空项\n",
    "        return sorted(authors)\n",
    "    return []\n",
    "\n",
    "def merge_name_variants(authors):\n",
    "    \"\"\"合并同一作者的不同写法，如 Juan Maldacena 和 Juan M. Maldacena\"\"\"\n",
    "    name_variants = {}\n",
    "    \n",
    "    for author in authors:\n",
    "        base_name = re.sub(r'\\b[A-Z]\\.', '', author).strip()  # 移除首字母缩写\n",
    "        if base_name in name_variants:\n",
    "            name_variants[base_name].add(author)\n",
    "        else:\n",
    "            name_variants[base_name] = {author}\n",
    "    \n",
    "    return [max(variants, key=len) for variants in name_variants.values()]\n",
    "\n",
    "def resolve_abbreviations(authors):\n",
    "    \"\"\"检查是否有缩写和全称重复的情况，若有，则保留全称\"\"\"\n",
    "    full_names = set()\n",
    "    abbreviations = set()\n",
    "    \n",
    "    for author in authors:\n",
    "        if '.' in author:\n",
    "            abbreviations.add(author)\n",
    "        else:\n",
    "            full_names.add(author)\n",
    "    \n",
    "    # 只保留非缩写形式的作者名\n",
    "    final_authors = full_names.union(a for a in abbreviations if a not in full_names)\n",
    "    return list(final_authors)\n",
    "\n",
    "def extract_arxiv_info(abs_file):\n",
    "    with open(abs_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    paper_id_match = re.search(r'Paper:\\s*hep-th/(\\d+)', content)\n",
    "    title_match = re.search(r'Title:\\s*(.+)', content)\n",
    "    authors = extract_author(content)\n",
    "    \n",
    "    if paper_id_match and title_match and authors:\n",
    "        return paper_id_match.group(1), {\n",
    "            \"title\": title_match.group(1),\n",
    "            \"authors\": authors\n",
    "        }\n",
    "    return None\n",
    "\n",
    "def process_abstracts(directory):\n",
    "    arxiv_data = {}\n",
    "    authors_set = set()\n",
    "    \n",
    "    for year in os.listdir(directory):\n",
    "        year_path = os.path.join(directory, year)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue\n",
    "        \n",
    "        for file in os.listdir(year_path):\n",
    "            if file.endswith(\".abs\"):\n",
    "                abs_path = os.path.join(year_path, file)\n",
    "                result = extract_arxiv_info(abs_path)\n",
    "                if result:\n",
    "                    arxiv_id, data = result\n",
    "                    arxiv_data[arxiv_id] = data\n",
    "                    authors_set.update(data[\"authors\"])\n",
    "    \n",
    "    with open(\"arxiv_data.json\", \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(arxiv_data, json_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    sorted_authors = sorted(authors_set)\n",
    "    with open(\"authors_list.csv\", \"w\", encoding=\"utf-8\", newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"Author\"])\n",
    "        for author in sorted_authors:\n",
    "            writer.writerow([author])\n",
    "    \n",
    "    print(\"Extraction completed. Data saved to arxiv_data.json and authors_list.csv\")\n",
    "\n",
    "# 示例调用\n",
    "process_abstracts(\"./cit-HepTh-abstracts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "example of Cit-HepTh.txt:\n",
    "# Directed graph (each unordered pair of nodes is saved once): Cit-HepTh.txt \n",
    "# Paper citation network of Arxiv High Energy Physics Theory category\n",
    "# Nodes: 27770 Edges: 352807\n",
    "# FromNodeId\tToNodeId\n",
    "1001\t9304045\n",
    "1001\t9308122\n",
    "1001\t9309097\n",
    "1001\t9311042\n",
    "1001\t9401139\n",
    "\n",
    "for each paper, we will add a new attribute of list that shows all the papers that this paper cite, with its arxiv number.\n",
    "We will add it iff both papers of a citation are in the paper. And save it to arxiv_papers.json.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#load data from arxiv_data.json\n",
    "with open(\"arxiv_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "\n",
    "# Load the citation data\n",
    "citations = {}\n",
    "with open(\"Cit-HepTh.txt\", \"r\") as f:\n",
    "    # Skip the lines with comments\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line.startswith(\"#\"):\n",
    "            break\n",
    "    for line in f:\n",
    "        source, target = map(int, line.strip().split())\n",
    "        source = str(source)\n",
    "        target = str(target)\n",
    "        if source in papers and target in papers:\n",
    "            if source not in citations:\n",
    "                citations[source] = []\n",
    "            citations[source].append(target)\n",
    "\n",
    "# Add the citation of the papers and the paper cited by the papers\n",
    "for paper_id, data in papers.items():\n",
    "    if paper_id in citations:\n",
    "        data[\"citations\"] = citations[paper_id]\n",
    "    else:\n",
    "        data[\"citations\"] = []\n",
    "\n",
    "\n",
    "# Save the updated data\n",
    "with open(\"arxiv_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(papers, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 2620\n",
      "Number of edges: 1331\n"
     ]
    }
   ],
   "source": [
    "# create a graph with authors as nodes and collaborations as edges\n",
    "import networkx as nx\n",
    "Ghep = nx.Graph()\n",
    "for paper in papers.values():\n",
    "    for author in paper[\"authors\"]:\n",
    "        if author not in Ghep:\n",
    "            Ghep.add_node(author, papers=[])\n",
    "            # Add the paper with its arxiv numbres to the author's list \n",
    "        Ghep.nodes[author][\"papers\"].append(paper[\"title\"])\n",
    "\n",
    "    for author1 in paper[\"authors\"]:\n",
    "        for author2 in paper[\"authors\"]:\n",
    "            if author1 != author2:\n",
    "                if not Ghep.has_edge(author1, author2):\n",
    "                    Ghep.add_edge(author1, author2, weight=0)\n",
    "                Ghep[author1][author2][\"weight\"] += 1\n",
    "\n",
    "print(f\"Number of nodes: {Ghep.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {Ghep.number_of_edges()}\")\n",
    "\n",
    "#print all the authors with the number of collaborations in authors.csv in alphabetical order\n",
    "with open(\"authors.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Author,Number of collaborations\\n\")\n",
    "    for author in sorted(Ghep.nodes):\n",
    "        f.write(f\"{author}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1706\n",
      "201\n",
      "1740\n"
     ]
    }
   ],
   "source": [
    "# find communities in the graph\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "\n",
    "# Louvain community detection\n",
    "lc_Ghep = louvain_communities(Ghep, seed=123)\n",
    "\n",
    "#print the community number\n",
    "print(len(lc_Ghep))\n",
    "# Clique percolation method\n",
    "from networkx.algorithms.community import k_clique_communities\n",
    "cp_Ghep= k_clique_communities(Ghep, 3)\n",
    "#print the community number\n",
    "print(len(list(cp_Ghep)))\n",
    "\n",
    "\n",
    "\n",
    "# label propagation algorithm\n",
    "from networkx.algorithms.community.label_propagation import label_propagation_communities\n",
    "lp_Ghep = list(label_propagation_communities(Ghep))\n",
    "#print the community number\n",
    "print(len(lp_Ghep))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most important communities:\n",
      "Community 1: 2 authors, 367.0 citations\n",
      "  Jaydeep Majumder\n",
      "  Ashoke Sen\n",
      "Community 2: 9 authors, 256.0 citations\n",
      "  Donam Youm\n",
      "  Ingo Gaida\n",
      "  Mirjam Cvetic\n",
      "  Kwanleung Chan\n",
      "  Vijay Balasubramanian\n",
      "  Lorenzo Cornalba\n",
      "  Dieter Luest\n",
      "  Frank Wilczek\n",
      "  Finn Larsen\n",
      "Community 3: 11 authors, 251.0 citations\n",
      "  Camillo Imbimbo\n",
      "  Steven S. Gubser\n",
      "  N. Itzhaki\n",
      "  Sunil Mukhi\n",
      "  Carlo M. Becchi\n",
      "  Edward Witten\n",
      "  Debashis Ghoshal\n",
      "  Igor R. Klebanov\n",
      "  Keshav Dasgupta\n",
      "  Akikazu Hashimoto\n",
      "  L. Susskind\n",
      "Community 4: 3 authors, 208.0 citations\n",
      "  Peter Langfelder\n",
      "  Zurab Kakushadze\n",
      "  Alberto Iglesias\n",
      "Community 5: 1 authors, 182.0 citations\n",
      "  John H. Schwarz\n",
      "Community 6: 6 authors, 182.0 citations\n",
      "  A.M. Polyakov\n",
      "  M. Cvetic\n",
      "  C.G. Callan\n",
      "  S.S. Gubser\n",
      "  A.A. Tseytlin\n",
      "  I.R. Klebanov\n",
      "Community 7: 22 authors, 149.0 citations\n",
      "  C. Sonnenschein\n",
      "  D. Freed\n",
      "  O. Aharony\n",
      "  E. Verlinde\n",
      "  I.Antoniadis\n",
      "  S. Theisen\n",
      "  G. Moore\n",
      "  S. Ferrara\n",
      "  A. Strominger\n",
      "  K.S. Narain\n",
      "  A. Klemm\n",
      "  S. Yankielowicz\n",
      "  S. Hosono\n",
      "  C. Vafa\n",
      "  R. Dijkgraaf\n",
      "  W. Lerche\n",
      "  S. Kachru\n",
      "  P. Mayr\n",
      "  J. A. Harvey\n",
      "  R. Minasian\n",
      "  Shingtung Yau\n",
      "  K. Benakli\n",
      "Community 8: 2 authors, 123.0 citations\n",
      "  Valeri V. Dvoeglazov\n",
      "  Sergei V. Khudyakov\n",
      "Community 9: 9 authors, 118.0 citations\n",
      "  Andrew Strominger\n",
      "  Jose D. Edelstein\n",
      "  Liat Maoz\n",
      "  Hong Liu\n",
      "  Carlos Nunez\n",
      "  Juan M. Maldacena\n",
      "  Juan Maldacena\n",
      "  Neil Lambert\n",
      "  Stephen Hawking\n",
      "Community 10: 7 authors, 117.0 citations\n",
      "  Miao Li\n",
      "  Hidetoshi Awata\n",
      "  Yihong Gao\n",
      "  Tamiaki Yoneya\n",
      "  Djordje Minic\n",
      "  Yongshi Wu\n",
      "  Antal Jevicki\n"
     ]
    }
   ],
   "source": [
    "#calculate the net citations numbers between communities directly from the arxiv_papers.json\n",
    "\n",
    "\"\"\"\n",
    "First few lines of the arxiv_papers.json:\n",
    "{\n",
    "    \"9203077\": {\n",
    "        \"title\": \"Finite W-algebras\",\n",
    "        \"authors\": [\n",
    "            \"T.Tjin\"\n",
    "        ],\n",
    "        \"citations\": 13,\n",
    "        \"cited_by\": []\n",
    "    },\n",
    "    \"9203063\": {\n",
    "        \"title\": \"The Spectrum of Sl(2, R)/U(1) Black Hole Conformal Field Theory\",\n",
    "        \"authors\": [\n",
    "            \"Dileep P. Jatkar\"\n",
    "        ],\n",
    "        \"citations\": 0,\n",
    "        \"cited_by\": []\n",
    "    },\n",
    "    \"9212146\": {\n",
    "\"\"\"\n",
    "communities = list(lc_Ghep)  # Use the Louvain communities\n",
    "# Load the papers data\n",
    "with open(\"arxiv_data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "# Create a dictionary of authors to communities\n",
    "author_community = {}\n",
    "for i, community in enumerate(communities):\n",
    "    for author in community:\n",
    "        author_community[author] = i\n",
    "\n",
    "# Calculate the net citations between communities\n",
    "net_citations = np.zeros((len(communities), len(communities)))\n",
    "for paper in papers.values():\n",
    "    if \"citations\" not in paper:\n",
    "        continue\n",
    "    source_community = author_community.get(paper[\"authors\"][0], -1)\n",
    "    for target in paper[\"citations\"]:\n",
    "        target_community = author_community.get(papers[target][\"authors\"][0], -1)\n",
    "        if source_community != -1 and target_community != -1:\n",
    "            net_citations[source_community, target_community] += 1\n",
    "\n",
    "\n",
    "# Find the most important communities by net citations/\n",
    "community_citations = net_citations.sum(axis=1)\n",
    "most_important_communities = np.argsort(community_citations)[::-1]  \n",
    "\n",
    "print(\"Most important communities:\")\n",
    "for i in range(10):\n",
    "    community = communities[most_important_communities[i]]\n",
    "    print(f\"Community {i + 1}: {len(community)} authors, {community_citations[most_important_communities[i]]} citations\")\n",
    "    for author in community:\n",
    "        print(f\"  {author}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1706\n",
      "Number of edges: 1246\n"
     ]
    }
   ],
   "source": [
    "#create a graph of communities as summation of the information of the authors named after the most important author in the community.\n",
    "# we will create a graph of communities as summation of the information of the authors named after the most important author in the community. \n",
    "# The number of citations as the weight of the edges.\n",
    "G_communities = nx.Graph()\n",
    "for i, community in enumerate(communities):\n",
    "    most_important_author = max(community, key=lambda x: len(papers[x][\"papers\"]) if x in papers else 0)\n",
    "    G_communities.add_node(i, name=most_important_author, size=len(community))\n",
    "    #add the number of members in the community as the size of the node\n",
    "    for author in community:\n",
    "        if author in papers:\n",
    "            G_communities.nodes[i][author] = papers[author][\"papers\"]\n",
    "    #add the total citations in the community\n",
    "    G_communities.nodes[i][\"total_citations\"] = community_citations[i]\n",
    "\n",
    "# Create the edges between communities based on the net citations\n",
    "for i in range(len(communities)):\n",
    "    for j in range(i + 1, len(communities)):\n",
    "        # Add the number of citations as the weight of the edge with weight of the number of citations\n",
    "        if net_citations[i][j] > 0:\n",
    "            G_communities.add_edge(i, j, weight=net_citations[i][j])\n",
    "\n",
    "print(f\"Number of nodes: {G_communities.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G_communities.number_of_edges()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed69bb899d4f479b909e58f46fbe53dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sigma(nx.Graph with 1,706 nodes and 1,246 edges)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the graph of G_communities with ipysigma\n",
    "\n",
    "# Assign colors to each communities based on the numbers of members of communities from red to blue\n",
    "\n",
    "for node in G_communities.nodes:\n",
    "    r = max(0, min(255, 255 - 20 * len(communities[node])))\n",
    "    g = max(0, min(255, 20 * len(communities[node])))\n",
    "    G_communities.nodes[node][\"colors\"] = f\"rgb({r}, O,{g})\"\n",
    "\n",
    "# assgin the color of the edges based on the citations of the community\n",
    "for i, j in G_communities.edges:\n",
    "    G_communities.edges[i, j][\"color\"] = f\"rgb({255 - 20 * net_citations[i, j]}, {20 * net_citations[i, j]}, 0)\"\n",
    "    \n",
    "# Assign the size of the nodes based on citations\n",
    "for node in G_communities.nodes:\n",
    "    G_communities.nodes[node][\"size\"] = G_communities.nodes[node][\"total_citations\"] / 100\n",
    "\n",
    "# Visualize with ipysigma\n",
    "sigma = Sigma(\n",
    "    G_communities,\n",
    "    node_color=\"colors\",\n",
    "    edge_color=\"color\",\n",
    "    node_label=\"name\",\n",
    "    node_size=\"size\",\n",
    "    label_font=\"cursive\",\n",
    "    default_edge_type=\"curve\"\n",
    ")\n",
    "\n",
    "sigma"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
