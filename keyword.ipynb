{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46a109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwe have already processed the extraction of the keywords, now we try to visualize\\nthe results\\n\\nthe first step is to integrate temp/keyword_clusters.json and temp/keywords_extracted.json\\n\\nand then we create a graph of paper based on the references relations in keyword.ipynb\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "we have already processed the extraction of the keywords, now we try to visualize\n",
    "the results\n",
    "\n",
    "the first step is to integrate temp/keyword_clusters.json and temp/keywords_extracted.json\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f14eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "\n",
    "import json\n",
    "import networkx as nx\n",
    "from ipysigma import Sigma\n",
    "import networkx as nx\n",
    "import networkx as nx\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import networkx as nx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda52364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we clean the temp/keywords_extracted.json using the temp/keyword_clusters.jsonl with\n",
    "\"\"\"\n",
    "   temp/keyword_clusters.json\n",
    "    \"vortex cells\": [\n",
    "        \"vortex cells\",\n",
    "        \"vortex filament\",\n",
    "        \"vortex filaments\",\n",
    "        \"vortex gas\"\n",
    "    ],\n",
    "\n",
    "    and\n",
    "    temp/keywords_extracted.jsonl\n",
    "   {\"paper_id\": \"0302109\", \"title\": \"The C-Deformation of Gluino and Non-planar Diagrams\", \"keywords\": [\"supersymmetric gauge theories\", \"genus partition function\", \"non - planar diagrams\"]}\n",
    "\"\"\"\n",
    "# and we save the result in a new file called temp/keywords_cleaned.json\n",
    "\n",
    "def clean_keywords(keywords_extracted_path, keyword_clusters_path, output_path='temp/keywords_cleaned.json'):\n",
    "    import json\n",
    "\n",
    "    # 加载关键词提取结果\n",
    "    with open(keywords_extracted_path, 'r') as f:\n",
    "        extracted_keywords = [json.loads(line) for line in f]\n",
    "\n",
    "    # 加载关键词簇映射\n",
    "    with open(keyword_clusters_path, 'r') as f:\n",
    "        keyword_clusters = json.load(f)\n",
    "\n",
    "    # 构建一个反向映射：每个同义词 -> 它的标准关键词（cluster key）\n",
    "    synonym_to_cluster = {}\n",
    "    for cluster_name, synonyms in keyword_clusters.items():\n",
    "        for synonym in synonyms:\n",
    "            synonym_to_cluster[synonym.lower()] = cluster_name  # 小写匹配\n",
    "\n",
    "    cleaned_keywords = {}\n",
    "\n",
    "    for paper in extracted_keywords:\n",
    "        paper_id = paper[\"paper_id\"]\n",
    "        title = paper[\"title\"]\n",
    "        raw_keywords = paper[\"keywords\"]\n",
    "\n",
    "        # 将所有关键词映射为标准关键词\n",
    "        standardized_keywords = set()\n",
    "        for kw in raw_keywords:\n",
    "            canonical = synonym_to_cluster.get(kw.lower(), kw)\n",
    "            standardized_keywords.add(canonical)\n",
    "\n",
    "        cleaned_keywords[paper_id] = {\n",
    "            \"title\": title,\n",
    "            \"keywords\": sorted(standardized_keywords)\n",
    "        }\n",
    "\n",
    "    # 保存结果\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(cleaned_keywords, f, indent=4)\n",
    "\n",
    "\n",
    "clean_keywords('temp/keywords_extracted.jsonl', 'temp/keyword_clusters.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5c29379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we use the cleaned keywords to create a graph of paper based on the references relations in Cit-HepTh.txt\n",
    "\"\"\"\n",
    "# Directed graph (each unordered pair of nodes is saved once): Cit-HepTh.txt \n",
    "# Paper citation network of Arxiv High Energy Physics Theory category\n",
    "#\n",
    "#the original data omits all the zeros in front of a node, we use the regex to correct it.\n",
    "#\n",
    "# Nodes: 27770 Edges: 0352807\n",
    "# FromNodeId\tToNodeId\n",
    "0001001\t9304045\n",
    "0001001\t9308122\n",
    "0001001\t9309097\n",
    "0001001\t9311042\n",
    "0001001\t9401139\n",
    "0001001\t9404151\n",
    "\"\"\"\n",
    "\n",
    "def create_graph_from_citation_file(citation_file):\n",
    "    \"\"\"\n",
    "    Create a graph from the citation file\n",
    "    \"\"\"\n",
    "    # load the citation file\n",
    "    with open(citation_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # iterate over the lines in the citation file\n",
    "    for line in lines[1:]:\n",
    "        # skip the lines with #\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        # split the line into two nodes\n",
    "        node1, node2 = line.strip().split('\\t')\n",
    "        # add the nodes to the graph\n",
    "        G.add_edge(node1, node2)\n",
    "        \n",
    "    # add information about the nodes from the temp/keywords_cleaned.json\n",
    "    \"\"\"\n",
    "        \"9211012\": {\n",
    "        \"title\": \"Some Constant Solutions to Zamolodchikov's Tetrahedron Equations\",\n",
    "        \"keywords\": [\n",
    "            \"baxter equation\",\n",
    "            \"tetrahedron equations\",\n",
    "            \"zamolodchikov\"\n",
    "        ]\n",
    "    },\n",
    "    \"\"\"\n",
    "\n",
    "    with open('temp/keywords_cleaned.json', 'r') as f:\n",
    "        keywords_cleaned = json.load(f)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        # add the node to the graph\n",
    "        if node in keywords_cleaned:\n",
    "            # add the title and keywords to the node\n",
    "            G.nodes[node]['title'] = keywords_cleaned[node]['title']\n",
    "            G.nodes[node]['keywords'] = keywords_cleaned[node]['keywords']\n",
    "        else:\n",
    "            # if the node is not in the keywords_cleaned.json, set the title and keywords to None\n",
    "            G.nodes[node]['title'] = None\n",
    "            G.nodes[node]['keywords'] = None\n",
    "\n",
    "    return G\n",
    "\n",
    "citation_file = 'assets/Cit-HepTh.txt'\n",
    "G = create_graph_from_citation_file(citation_file)\n",
    "\n",
    "# sigma = Sigma(G)\n",
    "# sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "209195a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper ID: 9905111, Title: Large N Field Theories, String Theory and Gravity, Betweenness Centrality: 0.1033\n",
      "Paper ID: 9810008, Title: Conformal Anomaly for Dilaton Coupled Theories from AdS/CFT Correspondence, Betweenness Centrality: 0.0800\n",
      "Paper ID: 0206223, Title: Constant Curvature Black Hole and Dual Field Theory, Betweenness Centrality: 0.0800\n",
      "Paper ID: 9509140, Title: Non-Perturbative Green's Functions in Theories with Extended Superconformal Symmetry, Betweenness Centrality: 0.0503\n",
      "Paper ID: 9607239, Title: Superconformal Ward Identities and N=2 Yang-Mills Theory, Betweenness Centrality: 0.0375\n",
      "Paper ID: 9803001, Title: Macroscopic strings as heavy quarks: Large-N gauge theory and anti-de Sitter supergravity, Betweenness Centrality: 0.0374\n",
      "Paper ID: 9912210, Title: Scalar Quartic Couplings in Type IIB Supergravity on $AdS_5\\times S^5$, Betweenness Centrality: 0.0313\n",
      "Paper ID: 9902121, Title: A Stress Tensor for Anti-de Sitter Gravity, Betweenness Centrality: 0.0197\n",
      "Paper ID: 9907085, Title: Some Cubic Couplings in Type IIB Supergravity on $AdS_5\\times S^5$ and Three-point Functions in SYM_4 at Large N, Betweenness Centrality: 0.0190\n",
      "Paper ID: 0003136, Title: The String Dual of a Confining Four-Dimensional Gauge Theory, Betweenness Centrality: 0.0170\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we can use the networkx library to calculate the betweenness centrality of the graph\n",
    "betweenness_approx = nx.betweenness_centrality(G, k=100, seed=42, normalized=True)\n",
    "\n",
    "\n",
    "# we can use the networkx library to calculate the betweenness centrality of the graph\n",
    "top10 = sorted(betweenness_approx.items(), key=lambda x: -x[1])[:10]\n",
    "for pid, score in top10:\n",
    "    # print id and title of the paper the title is in the keywords_cleaned.json\n",
    "    title = G.nodes[pid]['title'] if pid in G.nodes else 'Unknown'\n",
    "    \n",
    "    print(f'Paper ID: {pid}, Title: {title}, Betweenness Centrality: {score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d273aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    }
   ],
   "source": [
    "# we use algorithms to find the communities in the graph\n",
    "# find communities in the graph\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "\n",
    "# Louvain community detection\n",
    "lc_Ghep = louvain_communities(G,resolution=0.1, seed=123)\n",
    "\n",
    "#print the community number\n",
    "print(len(lc_Ghep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b9429bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852dcb3ca8144e86bc8561e9f6f6de68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sigma(nx.DiGraph with 27,770 nodes and 352,807 edges)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def assign_domains_by_cluster(G, n_domains=15):\n",
    "    # 提取节点列表\n",
    "    node_list = list(G.nodes())\n",
    "    # 把每个节点的关键词转为字符串\n",
    "    texts = [\" \".join(G.nodes[n].get(\"keywords\") or []) for n in node_list]\n",
    "\n",
    "    # TF-IDF 向量化\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "\n",
    "    # KMeans 聚类\n",
    "    kmeans = KMeans(n_clusters=n_domains, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # 给每个节点分配领域编号和颜色\n",
    "    domain_palette = [\n",
    "        \"#FF6F61\", \"#6B5B95\", \"#88B04B\", \"#F7CAC9\", \"#92A8D1\",\n",
    "        \"#955251\", \"#B565A7\", \"#009B77\", \"#DD4124\", \"#45B8AC\",\n",
    "        \"#EFC050\", \"#5B5EA6\", \"#9B2335\", \"#DFCFBE\", \"#BC243C\"\n",
    "    ]\n",
    "\n",
    "    for i, node in enumerate(node_list):\n",
    "        G.nodes[node][\"domain\"] = int(labels[i])\n",
    "        G.nodes[node][\"color\"] = domain_palette[int(labels[i]) % len(domain_palette)]\n",
    "\n",
    "    return G\n",
    "\n",
    "def extract_time_from_id(paper_id):\n",
    "    try:\n",
    "        year = int(paper_id[:2])\n",
    "        month = int(paper_id[2:4])\n",
    "        if year >= 90:\n",
    "            year += 1900\n",
    "        else:\n",
    "            year += 2000\n",
    "        return year, month\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def assign_positions_by_domain_time(G):\n",
    "    # 横向按领域排\n",
    "    domain_ids = sorted(set(nx.get_node_attributes(G, \"domain\").values()))\n",
    "    domain_x_base = {d: i * 10.0 for i, d in enumerate(domain_ids)}\n",
    "\n",
    "    # 暂存每个 domain, (year, month) 下的节点列表\n",
    "    bucket = defaultdict(list)\n",
    "    for node in G.nodes():\n",
    "        domain = G.nodes[node].get(\"domain\", 0)\n",
    "        year, month = extract_time_from_id(node)\n",
    "        if year and month:\n",
    "            bucket[(domain, year, month)].append(node)\n",
    "        else:\n",
    "            bucket[(domain, 0, 0)].append(node)  # fallback\n",
    "\n",
    "    # 为每组节点分配聚类排布\n",
    "    for (domain, year, month), nodes in bucket.items():\n",
    "        x_base = domain_x_base[domain]\n",
    "        y = year + (month - 1) / 12.0\n",
    "        for i, node in enumerate(nodes):\n",
    "            # 稍微左右分散一点，避免重叠\n",
    "            offset = (i - len(nodes) / 2) * 0.3\n",
    "            G.nodes[node][\"x\"] = x_base + offset\n",
    "            G.nodes[node][\"y\"] = y*20\n",
    "            # use the degree of the node to set the size\n",
    "            G.nodes[node][\"size\"] = G.degree(node) * 10\n",
    "\n",
    "G = assign_domains_by_cluster(G, n_domains=15)\n",
    "#add a new attribute of time to the nodes\n",
    "def assign_time_to_nodes(G):\n",
    "    for node in G.nodes():\n",
    "        year, month = extract_time_from_id(node)\n",
    "        if year and month:\n",
    "            G.nodes[node][\"time\"] = f\"{year:04d}-{month:02d}\"\n",
    "        else:\n",
    "            G.nodes[node][\"time\"] = \"Unknown\"\n",
    "assign_time_to_nodes(G)\n",
    "assign_positions_by_domain_time(G)\n",
    "\n",
    "sigma = Sigma(\n",
    "    G,\n",
    "    node_color=\"color\",\n",
    "    edge_color=\"color\",\n",
    "    node_label=\"title\",\n",
    "    node_size=\"size\",\n",
    "    default_edge_type=\"curve\"\n",
    ")\n",
    "sigma\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44ef1527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2547b65d4d7245a5a45198dae2ccade2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sigma(nx.DiGraph with 146,919 nodes and 146,909 edges)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_core_id(pid):\n",
    "    return pid.split(\"_\")[0]\n",
    "\n",
    "def extract_time_from_id(paper_id):\n",
    "    try:\n",
    "        core_id = get_core_id(paper_id)\n",
    "        if len(core_id) != 7 or not core_id[:4].isdigit():\n",
    "            return None\n",
    "        year = int(core_id[:2])\n",
    "        month = int(core_id[2:4])\n",
    "        year += 1900 if year >= 90 else 2000\n",
    "        return datetime.datetime(year, month, 1)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_time(node):\n",
    "    return extract_time_from_id(node)\n",
    "\n",
    "def build_subtree(G, top10_ids, direction=\"forward\", x_spacing=6000, d=250, delta=8, max_month_diff=120):\n",
    "    G_sub = nx.DiGraph()\n",
    "    core_top10_ids = {get_core_id(tid) for tid in top10_ids}\n",
    "\n",
    "\n",
    "    align_year = 2003\n",
    "    align_month = 4\n",
    "\n",
    "    for i, root_id in enumerate(top10_ids):\n",
    "        root_time = extract_time(root_id)\n",
    "        if root_time is None:\n",
    "            continue\n",
    "\n",
    "        # 计算偏移，使得 align_date 的 y = 0\n",
    "        tree_offset_y = d * ((root_time.year - align_year) * 12 + (root_time.month - align_month))\n",
    "\n",
    "        x_center = i * x_spacing\n",
    "        root_time = extract_time(root_id)\n",
    "        if root_time is None:\n",
    "            continue\n",
    "\n",
    "        root_clone = root_id\n",
    "        G_sub.add_node(\n",
    "            root_clone,\n",
    "            title=G.nodes[root_id].get(\"title\", root_id),\n",
    "            x=x_center,\n",
    "            y=-tree_offset_y,\n",
    "            color=G.nodes[root_id].get(\"color\", \"gray\"),\n",
    "            size=10\n",
    "        )\n",
    "\n",
    "        layer_counts = defaultdict(int)\n",
    "        visited = set()\n",
    "        queue = deque([(root_id, root_clone)])\n",
    "\n",
    "        while queue:\n",
    "            orig_current, current_id = queue.popleft()\n",
    "            current_time = extract_time(orig_current)\n",
    "            if current_time is None:\n",
    "                continue\n",
    "\n",
    "            neighbors = (\n",
    "                G.successors(orig_current) if direction == \"forward\"\n",
    "                else G.predecessors(orig_current)\n",
    "            )\n",
    "\n",
    "            for neighbor in neighbors:\n",
    "                core_neighbor = get_core_id(neighbor)\n",
    "                if (neighbor, root_id) in visited or core_neighbor in core_top10_ids:\n",
    "                    continue\n",
    "                visited.add((neighbor, root_id))\n",
    "\n",
    "                if 'title' not in G.nodes[neighbor] or not G.nodes[neighbor]['title'].strip():\n",
    "                    continue\n",
    "                neighbor_time = extract_time(neighbor)\n",
    "                if neighbor_time is None:\n",
    "                    continue\n",
    "\n",
    "                delta_months = (neighbor_time.year - root_time.year) * 12 + (neighbor_time.month - root_time.month)\n",
    "\n",
    "                if direction == \"forward\" and delta_months >= 0:\n",
    "                    continue\n",
    "                if direction == \"backward\" and delta_months <= 0:\n",
    "                    continue\n",
    "                if abs(delta_months) > max_month_diff:\n",
    "                    continue\n",
    "\n",
    "                y_base = -d * delta_months\n",
    "                y_offset = 0\n",
    "\n",
    "                if neighbor_time.year == current_time.year and neighbor_time.month == current_time.month:\n",
    "                    y_offset = +d * 0.2 if direction == \"forward\" else -d * 0.2\n",
    "\n",
    "                y = y_base + y_offset - tree_offset_y\n",
    "\n",
    "                n_in_layer = layer_counts[delta_months]\n",
    "                x = x_center + delta * n_in_layer * ((-1) ** n_in_layer)\n",
    "                layer_counts[delta_months] += 1\n",
    "\n",
    "                node_id = f\"{neighbor}_from_{root_id}\"\n",
    "\n",
    "                if node_id not in G_sub:\n",
    "                    G_sub.add_node(\n",
    "                        node_id,\n",
    "                        title=G.nodes[neighbor].get(\"title\", neighbor),\n",
    "                        x=x,\n",
    "                        y=y,\n",
    "                        color=G.nodes[neighbor].get(\"color\", \"gray\"),\n",
    "                        size=5,\n",
    "                        time=neighbor_time.strftime(\"%Y-%m\")\n",
    "                    )\n",
    "\n",
    "                if direction == \"forward\":\n",
    "                    G_sub.add_edge(current_id, node_id, color=\"gray\")\n",
    "                else:\n",
    "                    G_sub.add_edge(node_id, current_id, color=\"gray\")\n",
    "\n",
    "                queue.append((neighbor, node_id))\n",
    "\n",
    "    return G_sub\n",
    "\n",
    "def build_recursive_temporal_tree(G, top10_ids, **kwargs):\n",
    "    G_forward = build_subtree(G, top10_ids, direction=\"forward\", **kwargs)\n",
    "    G_backward = build_subtree(G, top10_ids, direction=\"backward\", **kwargs)\n",
    "    return nx.compose(G_forward, G_backward)\n",
    "\n",
    "\n",
    "\n",
    "top10_ids = [pid for pid, _ in top10]\n",
    "\n",
    "G_structured = build_recursive_temporal_tree(G, top10_ids)\n",
    "\n",
    "sigma = Sigma(\n",
    "    G_structured,\n",
    "    node_color=\"color\",\n",
    "    edge_color=\"color\",\n",
    "    node_label=\"title\",\n",
    "    node_size=\"size\",\n",
    "    default_edge_type=\"curve\"\n",
    ")\n",
    "sigma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
