{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741e720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46a109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwe have already processed the extraction of the keywords, now we try to visualize\\nthe results\\n\\nthe first step is to integrate keyword_clusters.json and keywords_extracted.json\\n\\nand then we create a graph of paper based on the references relations in keyword.ipynb\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "we have already processed the extraction of the keywords, now we try to visualize\n",
    "the results\n",
    "\n",
    "the first step is to integrate temp/keyword_clusters.json and temp/keywords_extracted.json\n",
    "\n",
    "and then we create a graph of paper based on the references relations in keyword.ipynb\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f14eeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "from ipysigma import Sigma\n",
    "import networkx as nx\n",
    "import networkx as nx\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda52364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we clean the temp/keywords_extracted.json using the temp/keyword_clusters.jsonl with\n",
    "\"\"\"\n",
    "   temp/keyword_clusters.json\n",
    "    \"vortex cells\": [\n",
    "        \"vortex cells\",\n",
    "        \"vortex filament\",\n",
    "        \"vortex filaments\",\n",
    "        \"vortex gas\"\n",
    "    ],\n",
    "\n",
    "    and\n",
    "    temp/keywords_extracted.jsonl\n",
    "   {\"paper_id\": \"0302109\", \"title\": \"The C-Deformation of Gluino and Non-planar Diagrams\", \"keywords\": [\"supersymmetric gauge theories\", \"genus partition function\", \"non - planar diagrams\"]}\n",
    "\"\"\"\n",
    "# and we save the result in a new file called temp/keywords_cleaned.json\n",
    "\n",
    "def clean_keywords(keywords_extracted_path, keyword_clusters_path, output_path='temp/keywords_cleaned.json'):\n",
    "    import json\n",
    "\n",
    "    # 加载关键词提取结果\n",
    "    with open(keywords_extracted_path, 'r') as f:\n",
    "        extracted_keywords = [json.loads(line) for line in f]\n",
    "\n",
    "    # 加载关键词簇映射\n",
    "    with open(keyword_clusters_path, 'r') as f:\n",
    "        keyword_clusters = json.load(f)\n",
    "\n",
    "    # 构建一个反向映射：每个同义词 -> 它的标准关键词（cluster key）\n",
    "    synonym_to_cluster = {}\n",
    "    for cluster_name, synonyms in keyword_clusters.items():\n",
    "        for synonym in synonyms:\n",
    "            synonym_to_cluster[synonym.lower()] = cluster_name  # 小写匹配\n",
    "\n",
    "    cleaned_keywords = {}\n",
    "\n",
    "    for paper in extracted_keywords:\n",
    "        paper_id = paper[\"paper_id\"]\n",
    "        title = paper[\"title\"]\n",
    "        raw_keywords = paper[\"keywords\"]\n",
    "\n",
    "        # 将所有关键词映射为标准关键词\n",
    "        standardized_keywords = set()\n",
    "        for kw in raw_keywords:\n",
    "            canonical = synonym_to_cluster.get(kw.lower(), kw)\n",
    "            standardized_keywords.add(canonical)\n",
    "\n",
    "        cleaned_keywords[paper_id] = {\n",
    "            \"title\": title,\n",
    "            \"keywords\": sorted(standardized_keywords)\n",
    "        }\n",
    "\n",
    "    # 保存结果\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(cleaned_keywords, f, indent=4)\n",
    "\n",
    "\n",
    "clean_keywords('temp/keywords_extracted.jsonl', 'temp/keyword_clusters.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c29379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we use the cleaned keywords to create a graph of paper based on the references relations in Cit-HepTh.txt\n",
    "\"\"\"\n",
    "# Directed graph (each unordered pair of nodes is saved once): Cit-HepTh.txt \n",
    "# Paper citation network of Arxiv High Energy Physics Theory category\n",
    "#\n",
    "#the original data omits all the zeros in front of a node, we use the regex to correct it.\n",
    "#\n",
    "# Nodes: 27770 Edges: 0352807\n",
    "# FromNodeId\tToNodeId\n",
    "0001001\t9304045\n",
    "0001001\t9308122\n",
    "0001001\t9309097\n",
    "0001001\t9311042\n",
    "0001001\t9401139\n",
    "0001001\t9404151\n",
    "\"\"\"\n",
    "\n",
    "def create_graph_from_citation_file(citation_file):\n",
    "    \"\"\"\n",
    "    Create a graph from the citation file\n",
    "    \"\"\"\n",
    "    # load the citation file\n",
    "    with open(citation_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # iterate over the lines in the citation file\n",
    "    for line in lines[1:]:\n",
    "        # skip the lines with #\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        # split the line into two nodes\n",
    "        node1, node2 = line.strip().split('\\t')\n",
    "        # add the nodes to the graph\n",
    "        G.add_edge(node1, node2)\n",
    "        \n",
    "    # add information about the nodes from the temp/keywords_cleaned.json\n",
    "    \"\"\"\n",
    "        \"9211012\": {\n",
    "        \"title\": \"Some Constant Solutions to Zamolodchikov's Tetrahedron Equations\",\n",
    "        \"keywords\": [\n",
    "            \"baxter equation\",\n",
    "            \"tetrahedron equations\",\n",
    "            \"zamolodchikov\"\n",
    "        ]\n",
    "    },\n",
    "    \"\"\"\n",
    "\n",
    "    with open('temp/keywords_cleaned.json', 'r') as f:\n",
    "        keywords_cleaned = json.load(f)\n",
    "\n",
    "    for node in G.nodes():\n",
    "        # add the node to the graph\n",
    "        if node in keywords_cleaned:\n",
    "            # add the title and keywords to the node\n",
    "            G.nodes[node]['title'] = keywords_cleaned[node]['title']\n",
    "            G.nodes[node]['keywords'] = keywords_cleaned[node]['keywords']\n",
    "        else:\n",
    "            # if the node is not in the keywords_cleaned.json, set the title and keywords to None\n",
    "            G.nodes[node]['title'] = None\n",
    "            G.nodes[node]['keywords'] = None\n",
    "\n",
    "    return G\n",
    "\n",
    "citation_file = 'Cit-HepTh.txt'\n",
    "G = create_graph_from_citation_file(citation_file)\n",
    "\n",
    "# sigma = Sigma(G)\n",
    "# sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "209195a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper ID: 9905111, Title: Large N Field Theories, String Theory and Gravity, Betweenness Centrality: 0.1033\n",
      "Paper ID: 9810008, Title: Conformal Anomaly for Dilaton Coupled Theories from AdS/CFT Correspondence, Betweenness Centrality: 0.0800\n",
      "Paper ID: 0206223, Title: Constant Curvature Black Hole and Dual Field Theory, Betweenness Centrality: 0.0800\n",
      "Paper ID: 9509140, Title: Non-Perturbative Green's Functions in Theories with Extended Superconformal Symmetry, Betweenness Centrality: 0.0503\n",
      "Paper ID: 9607239, Title: Superconformal Ward Identities and N=2 Yang-Mills Theory, Betweenness Centrality: 0.0375\n",
      "Paper ID: 9803001, Title: Macroscopic strings as heavy quarks: Large-N gauge theory and anti-de Sitter supergravity, Betweenness Centrality: 0.0374\n",
      "Paper ID: 9912210, Title: Scalar Quartic Couplings in Type IIB Supergravity on $AdS_5\\times S^5$, Betweenness Centrality: 0.0313\n",
      "Paper ID: 9902121, Title: A Stress Tensor for Anti-de Sitter Gravity, Betweenness Centrality: 0.0197\n",
      "Paper ID: 9907085, Title: Some Cubic Couplings in Type IIB Supergravity on $AdS_5\\times S^5$ and Three-point Functions in SYM_4 at Large N, Betweenness Centrality: 0.0190\n",
      "Paper ID: 0003136, Title: The String Dual of a Confining Four-Dimensional Gauge Theory, Betweenness Centrality: 0.0170\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# we can use the networkx library to calculate the betweenness centrality of the graph\n",
    "betweenness_approx = nx.betweenness_centrality(G, k=100, seed=42, normalized=True)\n",
    "\n",
    "\n",
    "# we can use the networkx library to calculate the betweenness centrality of the graph\n",
    "top10 = sorted(betweenness_approx.items(), key=lambda x: -x[1])[:10]\n",
    "for pid, score in top10:\n",
    "    # print id and title of the paper the title is in the keywords_cleaned.json\n",
    "    title = G.nodes[pid]['title'] if pid in G.nodes else 'Unknown'\n",
    "    \n",
    "    print(f'Paper ID: {pid}, Title: {title}, Betweenness Centrality: {score:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d273aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    }
   ],
   "source": [
    "# we use algorithms to find the communities in the graph\n",
    "# find communities in the graph\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "\n",
    "# Louvain community detection\n",
    "lc_Ghep = louvain_communities(G,resolution=0.1, seed=123)\n",
    "\n",
    "#print the community number\n",
    "print(len(lc_Ghep))\n",
    "\n",
    "\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "\n",
    "def search_resolution_for_community_count(G, target_min=10, target_max=20, trials=20):\n",
    "    import numpy as np\n",
    "    results = []\n",
    "    for res in np.linspace(0.01, 5, trials):\n",
    "        communities = louvain_communities(G, resolution=res, seed=42)\n",
    "        n = len(communities)\n",
    "        results.append((res, n))\n",
    "        print(f\"resolution={res:.2f} -> {n} communities\")\n",
    "        if target_min <= n <= target_max:\n",
    "            print(f\"✅ Acceptable: resolution={res:.2f}, communities={n}\")\n",
    "    return results\n",
    "\n",
    "#results = search_resolution_for_community_count(G, target_min=10, target_max=20, trials=50)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9429bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def assign_domains_by_cluster(G, n_domains=15):\n",
    "    # 提取节点列表\n",
    "    node_list = list(G.nodes())\n",
    "    # 把每个节点的关键词转为字符串\n",
    "    texts = [\" \".join(G.nodes[n].get(\"keywords\") or []) for n in node_list]\n",
    "\n",
    "    # TF-IDF 向量化\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "\n",
    "    # KMeans 聚类\n",
    "    kmeans = KMeans(n_clusters=n_domains, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # 给每个节点分配领域编号和颜色\n",
    "    domain_palette = [\n",
    "        \"#FF6F61\", \"#6B5B95\", \"#88B04B\", \"#F7CAC9\", \"#92A8D1\",\n",
    "        \"#955251\", \"#B565A7\", \"#009B77\", \"#DD4124\", \"#45B8AC\",\n",
    "        \"#EFC050\", \"#5B5EA6\", \"#9B2335\", \"#DFCFBE\", \"#BC243C\"\n",
    "    ]\n",
    "\n",
    "    for i, node in enumerate(node_list):\n",
    "        G.nodes[node][\"domain\"] = int(labels[i])\n",
    "        G.nodes[node][\"color\"] = domain_palette[int(labels[i]) % len(domain_palette)]\n",
    "\n",
    "    return G\n",
    "\n",
    "def extract_time_from_id(paper_id):\n",
    "    try:\n",
    "        year = int(paper_id[:2])\n",
    "        month = int(paper_id[2:4])\n",
    "        if year >= 90:\n",
    "            year += 1900\n",
    "        else:\n",
    "            year += 2000\n",
    "        return year, month\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "def assign_positions_by_domain_time(G):\n",
    "    # 横向按领域排\n",
    "    domain_ids = sorted(set(nx.get_node_attributes(G, \"domain\").values()))\n",
    "    domain_x_base = {d: i * 10.0 for i, d in enumerate(domain_ids)}\n",
    "\n",
    "    # 暂存每个 domain, (year, month) 下的节点列表\n",
    "    bucket = defaultdict(list)\n",
    "    for node in G.nodes():\n",
    "        domain = G.nodes[node].get(\"domain\", 0)\n",
    "        year, month = extract_time_from_id(node)\n",
    "        if year and month:\n",
    "            bucket[(domain, year, month)].append(node)\n",
    "        else:\n",
    "            bucket[(domain, 0, 0)].append(node)  # fallback\n",
    "\n",
    "    # 为每组节点分配聚类排布\n",
    "    for (domain, year, month), nodes in bucket.items():\n",
    "        x_base = domain_x_base[domain]\n",
    "        y = year + (month - 1) / 12.0\n",
    "        for i, node in enumerate(nodes):\n",
    "            # 稍微左右分散一点，避免重叠\n",
    "            offset = (i - len(nodes) / 2) * 0.3\n",
    "            G.nodes[node][\"x\"] = x_base + offset\n",
    "            G.nodes[node][\"y\"] = y*20\n",
    "            # use the degree of the node to set the size\n",
    "            G.nodes[node][\"size\"] = G.degree(node) * 10\n",
    "\n",
    "G = assign_domains_by_cluster(G, n_domains=15)\n",
    "#add a new attribute of time to the nodes\n",
    "def assign_time_to_nodes(G):\n",
    "    for node in G.nodes():\n",
    "        year, month = extract_time_from_id(node)\n",
    "        if year and month:\n",
    "            G.nodes[node][\"time\"] = f\"{year:04d}-{month:02d}\"\n",
    "        else:\n",
    "            G.nodes[node][\"time\"] = \"Unknown\"\n",
    "assign_time_to_nodes(G)\n",
    "assign_positions_by_domain_time(G)\n",
    "\n",
    "sigma = Sigma(\n",
    "    G,\n",
    "    node_color=\"color\",\n",
    "    edge_color=\"color\",\n",
    "    node_label=\"title\",\n",
    "    node_size=\"size\",\n",
    "    default_edge_type=\"curve\"\n",
    ")\n",
    "#sigma\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44ef1527",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 133\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mcompose(G_forward, G_backward)\n\u001b[1;32m    131\u001b[0m top10_ids \u001b[38;5;241m=\u001b[39m [pid \u001b[38;5;28;01mfor\u001b[39;00m pid, _ \u001b[38;5;129;01min\u001b[39;00m top10]\n\u001b[0;32m--> 133\u001b[0m G_structured \u001b[38;5;241m=\u001b[39m build_recursive_temporal_tree(G, top10_ids)\n\u001b[1;32m    135\u001b[0m sigma\u001b[38;5;241m=\u001b[39mSigma(\n\u001b[1;32m    136\u001b[0m     G_structured,\n\u001b[1;32m    137\u001b[0m     node_color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    141\u001b[0m     default_edge_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcurve\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m )\n\u001b[1;32m    143\u001b[0m sigma\n",
      "Cell \u001b[0;32mIn[36], line 127\u001b[0m, in \u001b[0;36mbuild_recursive_temporal_tree\u001b[0;34m(G, top10_ids, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m G_forward \u001b[38;5;241m=\u001b[39m build_subtree(G, top10_ids, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    126\u001b[0m G_backward \u001b[38;5;241m=\u001b[39m build_subtree(G, top10_ids, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mcompose(G_forward, G_backward)\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 27:3\u001b[0m, in \u001b[0;36margmap_compose_24\u001b[0;34m(G, H, backend, **backend_kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/networkx/utils/backends.py:633\u001b[0m, in \u001b[0;36m_dispatchable.__call__\u001b[0;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the result of the original function, or the backend function if\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03mthe backend is specified and that backend implements `func`.\"\"\"\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backends:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# Fast path if no backends are installed\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`\u001b[39;00m\n\u001b[1;32m    636\u001b[0m backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/networkx/algorithms/operators/binary.py:369\u001b[0m, in \u001b[0;36mcompose\u001b[0;34m(G, H)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;129m@nx\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatchable(graphs\u001b[38;5;241m=\u001b[39m_G_H, preserve_all_attrs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, returns_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompose\u001b[39m(G, H):\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compose graph G with H by combining nodes and edges into a single graph.\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m    The node sets and edges sets do not need to be disjoint.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    100.0\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mcompose_all([G, H])\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 31:3\u001b[0m, in \u001b[0;36margmap_compose_all_28\u001b[0;34m(graphs, backend, **backend_kwargs)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/networkx/utils/backends.py:633\u001b[0m, in \u001b[0;36m_dispatchable.__call__\u001b[0;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the result of the original function, or the backend function if\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03mthe backend is specified and that backend implements `func`.\"\"\"\u001b[39;00m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backends:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# Fast path if no backends are installed\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    635\u001b[0m \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`\u001b[39;00m\n\u001b[1;32m    636\u001b[0m backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/networkx/algorithms/operators/all.py:223\u001b[0m, in \u001b[0;36mcompose_all\u001b[0;34m(graphs)\u001b[0m\n\u001b[1;32m    221\u001b[0m     R\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mupdate(G\u001b[38;5;241m.\u001b[39mgraph)\n\u001b[1;32m    222\u001b[0m     R\u001b[38;5;241m.\u001b[39madd_nodes_from(G\u001b[38;5;241m.\u001b[39mnodes(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m--> 223\u001b[0m     R\u001b[38;5;241m.\u001b[39madd_edges_from(\n\u001b[1;32m    224\u001b[0m         G\u001b[38;5;241m.\u001b[39medges(keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m G\u001b[38;5;241m.\u001b[39mis_multigraph() \u001b[38;5;28;01melse\u001b[39;00m G\u001b[38;5;241m.\u001b[39medges(data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    225\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m R \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot apply compose_all to an empty list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/networkx/classes/digraph.py:795\u001b[0m, in \u001b[0;36mDiGraph.add_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pred[v] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madjlist_inner_dict_factory()\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_node[v] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_attr_dict_factory()\n\u001b[0;32m--> 795\u001b[0m datadict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adj[u]\u001b[38;5;241m.\u001b[39mget(v, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_attr_dict_factory())\n\u001b[1;32m    796\u001b[0m datadict\u001b[38;5;241m.\u001b[39mupdate(attr)\n\u001b[1;32m    797\u001b[0m datadict\u001b[38;5;241m.\u001b[39mupdate(dd)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import networkx as nx\n",
    "\n",
    "def get_core_id(pid):\n",
    "    return pid.split(\"_\")[0]\n",
    "\n",
    "def extract_time_from_id(paper_id):\n",
    "    try:\n",
    "        core_id = get_core_id(paper_id)\n",
    "        if len(core_id) != 7 or not core_id[:4].isdigit():\n",
    "            return None\n",
    "        year = int(core_id[:2])\n",
    "        month = int(core_id[2:4])\n",
    "        year += 1900 if year >= 90 else 2000\n",
    "        return datetime.datetime(year, month, 1)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_time(node):\n",
    "    return extract_time_from_id(node)\n",
    "\n",
    "def build_subtree(G, top10_ids, direction=\"forward\", x_spacing=6000, d=250, delta=8, max_month_diff=120):\n",
    "    G_sub = nx.DiGraph()\n",
    "    core_top10_ids = {get_core_id(tid) for tid in top10_ids}\n",
    "\n",
    "\n",
    "    align_year = 2003\n",
    "    align_month = 4\n",
    "\n",
    "    for i, root_id in enumerate(top10_ids):\n",
    "        root_time = extract_time(root_id)\n",
    "        if root_time is None:\n",
    "            continue\n",
    "\n",
    "        # 计算偏移，使得 align_date 的 y = 0\n",
    "        tree_offset_y = d * ((root_time.year - align_year) * 12 + (root_time.month - align_month))\n",
    "\n",
    "        x_center = i * x_spacing\n",
    "        root_time = extract_time(root_id)\n",
    "        if root_time is None:\n",
    "            continue\n",
    "\n",
    "        root_clone = root_id\n",
    "        G_sub.add_node(\n",
    "            root_clone,\n",
    "            title=G.nodes[root_id].get(\"title\", root_id),\n",
    "            x=x_center,\n",
    "            y=0,\n",
    "            color=G.nodes[root_id].get(\"color\", \"gray\"),\n",
    "            size=10\n",
    "        )\n",
    "\n",
    "        layer_counts = defaultdict(int)\n",
    "        visited = set()\n",
    "        queue = deque([(root_id, root_clone)])\n",
    "\n",
    "        while queue:\n",
    "            orig_current, current_id = queue.popleft()\n",
    "            current_time = extract_time(orig_current)\n",
    "            if current_time is None:\n",
    "                continue\n",
    "\n",
    "            neighbors = (\n",
    "                G.successors(orig_current) if direction == \"forward\"\n",
    "                else G.predecessors(orig_current)\n",
    "            )\n",
    "\n",
    "            for neighbor in neighbors:\n",
    "                core_neighbor = get_core_id(neighbor)\n",
    "                if (neighbor, root_id) in visited or core_neighbor in core_top10_ids:\n",
    "                    continue\n",
    "                visited.add((neighbor, root_id))\n",
    "\n",
    "                if 'title' not in G.nodes[neighbor] or not G.nodes[neighbor]['title'].strip():\n",
    "                    continue\n",
    "                neighbor_time = extract_time(neighbor)\n",
    "                if neighbor_time is None:\n",
    "                    continue\n",
    "\n",
    "                delta_months = (neighbor_time.year - root_time.year) * 12 + (neighbor_time.month - root_time.month)\n",
    "\n",
    "                if direction == \"forward\" and delta_months >= 0:\n",
    "                    continue\n",
    "                if direction == \"backward\" and delta_months <= 0:\n",
    "                    continue\n",
    "                if abs(delta_months) > max_month_diff:\n",
    "                    continue\n",
    "\n",
    "                y_base = -d * delta_months\n",
    "                y_offset = 0\n",
    "\n",
    "                if neighbor_time.year == current_time.year and neighbor_time.month == current_time.month:\n",
    "                    y_offset = +d * 0.2 if direction == \"forward\" else -d * 0.2\n",
    "\n",
    "                y = y_base + y_offset - tree_offset_y\n",
    "\n",
    "                n_in_layer = layer_counts[delta_months]\n",
    "                x = x_center + delta * n_in_layer * ((-1) ** n_in_layer)\n",
    "                layer_counts[delta_months] += 1\n",
    "\n",
    "                node_id = f\"{neighbor}_from_{root_id}\"\n",
    "\n",
    "                if node_id not in G_sub:\n",
    "                    G_sub.add_node(\n",
    "                        node_id,\n",
    "                        title=G.nodes[neighbor].get(\"title\", neighbor),\n",
    "                        x=x,\n",
    "                        y=y,\n",
    "                        color=G.nodes[neighbor].get(\"color\", \"gray\"),\n",
    "                        size=5,\n",
    "                        time=neighbor_time.strftime(\"%Y-%m\")\n",
    "                    )\n",
    "\n",
    "                if direction == \"forward\":\n",
    "                    G_sub.add_edge(current_id, node_id, color=\"gray\")\n",
    "                else:\n",
    "                    G_sub.add_edge(node_id, current_id, color=\"gray\")\n",
    "\n",
    "                queue.append((neighbor, node_id))\n",
    "\n",
    "    return G_sub\n",
    "\n",
    "def build_recursive_temporal_tree(G, top10_ids, **kwargs):\n",
    "    G_forward = build_subtree(G, top10_ids, direction=\"forward\", **kwargs)\n",
    "    G_backward = build_subtree(G, top10_ids, direction=\"backward\", **kwargs)\n",
    "    return nx.compose(G_forward, G_backward)\n",
    "\n",
    "\n",
    "\n",
    "top10_ids = [pid for pid, _ in top10]\n",
    "\n",
    "G_structured = build_recursive_temporal_tree(G, top10_ids)\n",
    "\n",
    "sigma=Sigma(\n",
    "    G_structured,\n",
    "    node_color=\"color\",\n",
    "    edge_color=\"color\",\n",
    "    node_label=\"title\",\n",
    "    node_size=\"size\",\n",
    "    default_edge_type=\"curve\"\n",
    ")\n",
    "sigma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
